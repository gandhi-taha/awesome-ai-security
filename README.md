# AI-Security – Resources and Frameworks [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome) [![Track Awesome List](https://www.trackawesomelist.com/badge.svg)](https://www.trackawesomelist.com/ottosulin/awesome-ai-security)
## 1. Learning Resources

### General ML/AI & Security
- [Damn Vulnerable MCP Server](https://github.com/harishsg993010/damn-vulnerable-MCP-server) – Deliberately vulnerable Model Context Protocol implementation for educational hacking.
- [MLSecOps Podcast](https://mlsecops.com/podcast)
- [GenAI Security Podcast](https://podcasts.apple.com/ph/podcast/the-genai-security-podcast/id1782916580)
- [OWASP ML Top 10](https://owasp.org/www-project-machine-learning-security-top-10/)
- [OWASP LLM Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
- [OWASP AI Security and Privacy Guide](https://owasp.org/www-project-ai-security-and-privacy-guide/)
- [OWASP WrongSecrets LLM Exercise](https://wrongsecrets.herokuapp.com/challenge/32)
- [NIST AIRC – AI Risk Resource Center](https://airc.nist.gov/Home)
- [MLSecOps Top 10 – Ethical ML Security](https://ethical.institute/security.html)

---

## 2. Governance

### 2.1 Frameworks and Standards
- [NIST AI Risk Management Framework (RMF)](https://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF)
- [ISO/IEC 42001: AI Management System](https://www.iso.org/standard/81230.html)
- [ISO/IEC 23894:2023 – AI Risk Guidance](https://www.iso.org/standard/77304.html)
- [Google Secure AI Framework (SAIF)](https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/)
- [ENISA Multilayer Cybersecurity Framework for AI](https://www.enisa.europa.eu/publications/multilayer-framework-for-good-cybersecurity-practices-for-ai)
- [OWASP LLM Applications Governance Checklist](https://genai.owasp.org/resource/llm-applications-cybersecurity-and-governance-checklist-english/)

### 2.2 Terminology & Taxonomies
- [NIST AI 100-2e2023 – Adversarial ML Taxonomy](https://csrc.nist.gov/publications/detail/white-paper/2023/03/08/adversarial-machine-learning-taxonomy-and-terminology/draft)
- [AVIDML](https://avidml.org/taxonomy/)
- [MITRE ATLAS – TTPs for Adversarial AI](https://atlas.mitre.org/)
- [ISO/IEC 22989:2022 – AI Concepts and Terminology](https://www.iso.org/standard/74296.html)

---

## 3. Offensive Tools & Frameworks

### 3.1 General Guides
- [OWASP GenAI Red Teaming Guide](https://genai.owasp.org/initiatives/#ai-redteaming)

### 3.2 Machine Learning Attack Tools
- [gym-malware](https://github.com/endgameinc/gym-malware)
- [Deep-pwning](https://github.com/cchio/deep-pwning)
- [Counterfit](https://github.com/Azure/counterfit)
- [DeepFool](https://github.com/lts4/deepfool)
- [Snaike-MLFlow](https://github.com/protectai/Snaike-MLflow)
- [HackingBuddyGPT](https://github.com/ipa-lab/hackingBuddyGPT)
- [Charcuterie](https://github.com/moohax/Charcuterie)
- [OffsecML Playbook](https://wiki.offsecml.com)
- [BadDiffusion](https://github.com/IBM/BadDiffusion)
- [adlib](https://github.com/vu-aml/adlib)
- [Adversarial Robustness Toolkit (ART)](https://research.ibm.com/projects/adversarial-robustness-toolbox)
- [cleverhans](https://github.com/cleverhans-lab/cleverhans)
- [foolbox](https://github.com/bethgelab/foolbox)

### 3.3 LLM Attack & Red Team Tools
- [garak](https://github.com/leondz/garak)
- [agentic_security](https://github.com/msoedov/agentic_security/)
- [Agentic Radar](https://github.com/splx-ai/agentic-radar)
- [llamator](https://github.com/LLAMATOR-Core/llamator)
- [whistleblower](https://github.com/Repello-AI/whistleblower)
- [LLMFuzzer](https://github.com/mnns/LLMFuzzer)
- [vigil-llm](https://github.com/deadbits/vigil-llm)
- [FuzzyAI](https://github.com/cyberark/FuzzyAI)
- [EasyJailbreak](https://github.com/EasyJailbreak/EasyJailbreak)
- [promptmap](https://github.com/utkusen/promptmap)
- [PyRIT](https://github.com/Azure/PyRIT)
- [PurpleLlama](https://github.com/meta-llama/PurpleLlama)
- [Giskard-AI](https://github.com/Giskard-AI/giskard)
- [promptfoo](https://github.com/promptfoo/promptfoo)
- [HouYi](https://github.com/LLMSecurity/HouYi)
- [llm-attacks](https://github.com/llm-attacks/llm-attacks)
- [Dropbox llm-security](https://github.com/dropbox/llm-security)
- [llm-security](https://github.com/greshake/llm-security)
- [OpenPromptInjection](https://github.com/liu00222/Open-Prompt-Injection)
- [Plexiglass](https://github.com/safellama/plexiglass)
- [ps-fuzz](https://github.com/prompt-security/ps-fuzz)
- [EasyEdit](https://github.com/zjunlp/EasyEdit)
- [spikee](https://github.com/WithSecureLabs/spikee)
- [Prompt Hacking Resources](https://github.com/PromptLabs/Prompt-Hacking-Resources)
- [mcp-injection-experiments](https://github.com/invariantlabs-ai/mcp-injection-experiments)

### 3.4 Offensive AI for Cybersecurity
- [AI Red Teaming Playground](https://github.com/microsoft/AI-Red-Teaming-Playground-Labs)
- [HackGPT](https://github.com/NoDataFound/hackGPT)
- [mcp-for-security](https://github.com/cyproxio/mcp-for-security)
- [cai](https://github.com/aliasrobotics/cai)
- [AIRTBench](https://github.com/dreadnode/AIRTBench-Code)

---

## 4. Defensive Tools & Frameworks

### 4.1 Guides & Frameworks
- [OWASP LLM and GenAI CoE Guide](https://genai.owasp.org/resource/llm-and-generative-ai-security-center-of-excellence-guide/)
- [Agentic AI Threats and Mitigations](https://genai.owasp.org/resource/agentic-ai-threats-and-mitigations/)
- [AI Security Solutions Landscape](https://genai.owasp.org/ai-security-solutions-landscape/)

### 4.2 Data & Governance
- [datasig](https://github.com/trailofbits/datasig)

### 4.3 Guardrails and Safety
- [Guardrails.ai](https://shreyar.github.io/guardrails/)
- [CodeGate](https://codegate.ai)
- [MCP-Security-Checklist](https://github.com/slowmist/MCP-Security-Checklist)
- [Awesome-MCP-Security](https://github.com/Puliczek/awesome-mcp-security)
- [LlamaFirewall](https://github.com/meta-llama/PurpleLlama/tree/main/LlamaFirewall)
- [awesome-ai-safety](https://github.com/hari-sikchi/awesome-ai-safety)
- [ZenGuard AI](https://github.com/ZenGuard-AI/fast-llm-security-guardrails)
- [llm-guard](https://github.com/protectai/llm-guard)
- [vibraniumdome](https://github.com/genia-dev/vibraniumdome)
- [mcp-guardian](https://github.com/eqtylab/mcp-guardian/)

### 4.4 Detection & Monitoring
- [modelscan](https://github.com/protectai/modelscan)
- [rebuff](https://github.com/woop/rebuff)
- [langkit](https://github.com/whylabs/langkit)
- [MCP-Scan](https://github.com/invariantlabs-ai/mcp-scan)

### 4.5 Privacy and Confidentiality
- [PyDP](https://github.com/OpenMined/PyDP)
- [Diffprivlib](https://github.com/IBM/differential-privacy-library)
- [PLOT4ai](https://plot4.ai/)
- [TenSEAL](https://github.com/OpenMined/TenSEAL)
- [SyMPC](https://github.com/OpenMined/SyMPC)
- [PyVertical](https://github.com/OpenMined/PyVertical)
- [Cloaked AI](https://ironcorelabs.com/products/cloaked-ai/)
- [PrivacyRaven](https://github.com/trailofbits/PrivacyRaven)

---

## 5. RAG (Retrieval-Augmented Generation) Security

### Risks
- Poisoned vector DBs (HijackRAG, adversarial retrievals)
- Data leakage via embeddings
- Prompt injection via context documents
- Embedding collisions to mislead retrieval

### Mitigations
- RBAC + least privilege access
- Embedding validation and sanitization
- Redaction of sensitive context at ingestion
- Prompt engineering with security patterns
- Guardrails + logging for inputs/outputs
- Vector DB scanning for anomalies

### References
- [ProtectAI RAG Security 101](https://protectai.com/blog/rag-security-101)
- [HijackRAG Attack Paper](https://arxiv.org/abs/2410.22832)
- [Amazon RAG + Bedrock Security](https://aws.amazon.com/blogs/machine-learning/protect-sensitive-data-in-rag-applications-with-amazon-bedrock/)
- [RAG Trends & Future](https://www.signitysolutions.com/blog/trends-in-active-retrieval-augmented-generation)
- [Agent-based AI vs RAG](https://www.techradar.com/pro/rag-is-dead-why-enterprises-are-shifting-to-agent-based-ai-architectures)

---

## 6. Current AI Security Trends

- Shift from RAG to agent-based architectures
- Focus on security-first AI product development
- Legal & compliance implications of AI hallucinations
- Model-level interpretability and explainability tools
- Differential privacy and encrypted training pipelines

---

